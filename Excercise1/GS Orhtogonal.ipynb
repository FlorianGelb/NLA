{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03f5018",
   "metadata": {},
   "source": [
    "Group: Ella Noomen, Florian Braun, Jacob McCalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabbfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5920c40b",
   "metadata": {},
   "source": [
    "In this exercise, we explore the numerical implications of two variations of the Gram-Schmidt algorithm for orthonormalising a set of vectors. Theorem 1.1.2 from the lecture slides describes this problem as follows: \"$x_1,..., x_k \\in \\mathbb{C}^n$ are linearly independent, then the Gram-Schmidt orthogonalization generates an orthonormal set $v1,..., v_k \\in \\mathbb{C}^n$\". The two variations of the Gram-Schmidt orthogonalisation are both iterative, and mathematically equivalent, but the critical difference lies in which set of vectors we use when computing the projection at each iteration. Function2 is known as the modified Gram-Schmidt algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f6a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60b7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function on the LHS of Algorithm 1.1.3 on the lecture slides\n",
    "def function1(X):\n",
    "    v = np.zeros(X.shape)\n",
    "    for j in range(X.shape[0]):\n",
    "        y = np.zeros(X.shape[1])\n",
    "        for i in range(j):\n",
    "            r = np.inner(X[j], v[i])\n",
    "            y += r * v[i]\n",
    "        v[j] = (X[j] - y)/np.linalg.norm(X[j] - y)\n",
    "    return v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f205b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function on the RHS of Algorithm 1.1.3 on the lecture slides\n",
    "def function2(X):\n",
    "    v = np.zeros(X.shape)\n",
    "    for j in range(X.shape[0]):\n",
    "        w = X[j]\n",
    "        for i in range(j):\n",
    "            r = np.inner(w, v[i])\n",
    "            w = w - r * v[i]\n",
    "        v[j] = (w)/np.linalg.norm(w)\n",
    "    return v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce50be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a Hilbert matrix of size nxn\n",
    "def GenerateHilbert(n):\n",
    "    H = np.zeros((n,n))\n",
    "    for i in range(1, n+1):\n",
    "        h = [1/(i + j) for j in range(n)]\n",
    "        H[i-1] = h\n",
    "    return H\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70adc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63328cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = GenerateHilbert(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796d1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = function1(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1a9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = function2(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7ae9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "R3 = function1(R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8d77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R4 = function1(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1c6c0",
   "metadata": {},
   "source": [
    "Since $R_1^{T}R_1$ and $R_2^{T}R_2$ are supposed to be equal to the identity matrix, the sum of all off diagonal entry should be zero, therefore $G_k := R_k^{T}R_k - \\mathbb{I}$, $\\Sigma_k = \\sum_{i=1}^{n}\\sum_{j=1}^{n} |g_{ij}^{k}|$ has to be zero. Due to numerical / rounding errors, this does not hold for the computed matrices above. Large $\\Sigma_k$ indicates, that the according matrix $R_k$ is not orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce26395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503893.94485053775"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(np.matmul(R1.T, R1) - np.identity(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707dbcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4127.781037977058"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(np.matmul(R2.T, R2) - np.identity(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119972c3",
   "metadata": {},
   "source": [
    "As we can see, the value when using function2 is significantly smaller than when using function1, which shows that this function returns a more orthogonal matrix, and hence function2 yields better results for the problem. This is because due to the fact that the orthogonalization inside the inner loop is not directly based on the according vector $x_j$. Rather, the numerical errors are taken into account during the process. This results in a \"more orthogonal\" matrix, which is the aim of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e244f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355194.0701877724"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(np.matmul(R3.T, R3) - np.identity(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4daa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5640844958381046e-07"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(np.matmul(R4.T, R4) - np.identity(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fd7da",
   "metadata": {},
   "source": [
    "Finally, matrices R3 and R4 are the result of reorthogonalising matrices R1 and R2. In this context, reorthogonalising means applying the function again to the output of running the algorithm the first time. As we can see, after reorthogonalisation, the values for $\\sum_{i=1}^{n}\\sum_{j=1}^{n} |g_{ij}^{k}|$ are (significantly) lower, suggesting that it is fruitful to apply to algorithm once more. In fact, in the case of R4, which was achieved by running with R2 as input, the sum is very close to 0, where 0 means the matrix is orthogonal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
